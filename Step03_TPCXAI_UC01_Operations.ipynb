{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version: 0.0.2  Updated date: 07/05/2024\n",
    "Conda Environment : py-snowpark_df_ml_fs-1.15.0_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Snowflake Feature Store - Â Customer Segmentation\n",
    "We will use the Use-Case to show how Snowflake Feature Store (and Model Registry) can be used to maintain & store features, retrieve them for training and perform micro-batch inference.\n",
    "\n",
    "In the development (TRAINING) enviroment we will \n",
    "- create FeatureViews in the Feature Store that maintain the required customer-behaviour features.\n",
    "- use these Features to train a model, and save the model in the Snowflake model-registry.\n",
    "- plot the clusters for the trained model to visually verify. \n",
    "\n",
    "In the production (SERVING) environment we will\n",
    "- re-create the FeatureViews on production data\n",
    "- generate an Inference FeatureView that uses the saved model to perform incremental inference\n",
    "\n",
    "# Model Operationalisation in Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Python packages\n",
    "import os\n",
    "import json\n",
    "\n",
    "# SNOWFLAKE\n",
    "# Snowpark\n",
    "from snowflake.snowpark import Session, DataFrame, Window, WindowSpec\n",
    "#from snowflake.snowpark import Analytics\n",
    "\n",
    "import snowflake.snowpark.functions as F\n",
    "\n",
    "# Snowflake Feature Store\n",
    "from snowflake.ml.feature_store import (FeatureView, Entity)\n",
    "\n",
    "# COMMON FUNCTIONS\n",
    "from useful_fns import check_and_update, formatSQL, create_ModelRegistry, create_FeatureStore, create_SF_Session\n",
    "\n",
    "# Feature Engineering Functions\n",
    "from feature_engineering_fns import uc01_load_data, uc01_pre_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Snowflake connection and database parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We point the `tpcxai_schema` variable to our `SERVING` schema, and this one change allows us to recreate the model development pipeline in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Schema (Environment)\n",
    "tpcxai_schema = 'SERVING'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_qs_role, tpcxai_database, tpcxai_training_schema, session, warehouse_env = create_SF_Session(tpcxai_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL OPERATIONALISATION\n",
    "* Recreate production Entity, FeatureViews in Production FeatureStore\n",
    "* Reuse the model fitted in development/training\n",
    "* Create new Inference FeatureView for incremental model-inference\n",
    "\n",
    "#### Setup Production Feature Store and references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Create/Reference Snowflake Model Registry - Common across Environments\n",
    "mr = create_ModelRegistry(session, tpcxai_database, 'MODEL_1')\n",
    "\n",
    "# Create/Reference Snowflake Feature Store for Training (Development) Environment\n",
    "fs = create_FeatureStore(session, tpcxai_database, f'''_{tpcxai_schema}_FEATURE_STORE''', warehouse_env)\n",
    "\n",
    "### Reference Data to Snowflake Dataframe Objects\n",
    "# Tables\n",
    "line_item_tbl              = '.'.join([tpcxai_database, tpcxai_schema,'LINEITEM'])\n",
    "order_tbl                  = '.'.join([tpcxai_database, tpcxai_schema,'ORDERS'])\n",
    "order_returns_tbl          = '.'.join([tpcxai_database, tpcxai_schema,'ORDER_RETURNS'])\n",
    "\n",
    "# Snowpark Dataframe\n",
    "line_item_sdf              = session.table(line_item_tbl)\n",
    "order_sdf                  = session.table(order_tbl)\n",
    "order_returns_sdf          = session.table(order_returns_tbl)\n",
    "print('''--- Created Data References ---''')\n",
    "\n",
    "# Model Name\n",
    "model_name = \"MODEL_1.UC01_SNOWFLAKEML_RF_REGRESSOR_MODELSKLEARN\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now rerun the exact same code that we lifted from our Development (TRAINING) process to recreate the Feature Engineering pipelines in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ORDER Entity\n",
    "if \"ORDER\" not in json.loads(fs.list_entities().select(F.to_json(F.array_agg(\"NAME\", True))).collect()[0][0]):\n",
    "    customer_entity = Entity(name=\"ORDER\", join_keys=[\"O_CUSTOMER_SK\"],desc=\"Primary Key for CUSTOMER ORDER\")\n",
    "    fs.register_entity(customer_entity)\n",
    "else:\n",
    "    customer_entity = fs.get_entity(\"ORDER\")\n",
    "print('''--- Created CUSTOMER Entity ---''')\n",
    "\n",
    "### Create & Load Source Data\n",
    "raw_data = uc01_load_data(order_sdf, line_item_sdf, order_returns_sdf)\n",
    "print('''--- Created Source Data ---''')\n",
    "\n",
    "### Create & Run Preprocessing Function \n",
    "preprocessed_data = uc01_pre_process(raw_data)\n",
    "print('''--- Created Preprocessed Data ---''')\n",
    "\n",
    "### Create Preprocessing FeatureView from Preprocess Dataframe (SQL)\n",
    "ppd_fv_name = \"FV_UC01_PREPROCESS\"\n",
    "ppd_fv_version = \"V_1\"\n",
    "# Define descriptions for the FeatureView's Features.  These will be added as comments to the database object\n",
    "preprocess_features_desc = { \"FREQUENCY\":\"Average yearly order frequency\",\n",
    "                             \"RETURN_RATIO\":\"Average of, Per Order Returns Ratio.  Per order returns ratio : total returns value / total order value\" }\n",
    "# Create Inference Feature View\n",
    "try:\n",
    "    # If FeatureView already exists just return the reference to it\n",
    "    fv_uc01_preprocess = fs.get_feature_view(name=ppd_fv_name,version=ppd_fv_version)\n",
    "except:\n",
    "    # Create the FeatureView instance\n",
    "    fv_uc01_preprocess_instance = FeatureView(\n",
    "        name=ppd_fv_name, \n",
    "        entities=[customer_entity], \n",
    "        feature_df=preprocessed_data,      # <- We can use the snowpark dataframe as-is from our Python\n",
    "        timestamp_col=\"LATEST_ORDER_DATE\",\n",
    "        refresh_freq=\"60 minute\",           # <- specifying optional refresh_freq creates FeatureView as Dynamic Table, else created as View.\n",
    "        refresh_mode=\"INCREMENTAL\",\n",
    "        desc=\"Features to support Use Case 01\").attach_feature_desc(preprocess_features_desc)\n",
    "\n",
    "    # Register the FeatureView instance.  Creates  object in Snowflake\n",
    "    fv_uc01_preprocess = fs.register_feature_view(\n",
    "        feature_view=fv_uc01_preprocess_instance, \n",
    "        version=ppd_fv_version, \n",
    "        block=True\n",
    "    )\n",
    "    print(f\"Feature View : {ppd_fv_name}_{ppd_fv_version} created in {tpcxai_schema}\")   \n",
    "else:\n",
    "    print(f\"Feature View : {ppd_fv_name}_{ppd_fv_version} already created in {tpcxai_schema}\")\n",
    "\n",
    "print('''---            DONE               ---''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Scheduled Inference Pipeline\n",
    "\n",
    "We now recreate our model inference process that will\n",
    "- retrieve the latest version of the model from the Model Registry.\n",
    "- read features from our feature pipeline (fv_uc01_preprocess featureview)\n",
    "- pass features & model into inference function (uc01_serve) and return inference dataframe\n",
    "- use inference dataframe to define a new FeatureView to maintain inference process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Create an Inference Dataframe that reads from our feature-engineering pipeline\n",
    "inference_input_sdf = fs.read_feature_view(fv_uc01_preprocess)\n",
    "inference_input_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get latest version of the model\n",
    "m = mr.get_model(model_name)\n",
    "m.show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get latest version of the model\n",
    "m = mr.get_model(model_name)\n",
    "latest_version = m.show_versions().iloc[-1]['name']\n",
    "mv = m.version(latest_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uc01_serve(featurevector, km4_purchases) -> DataFrame:\n",
    "    return km4_purchases.run(featurevector, function_name=\"predict\")\n",
    "\n",
    "# Test Inference process\n",
    "inference_result_sdf = uc01_serve(inference_input_sdf, mv)\n",
    "inference_result_sdf.sort(F.col('LATEST_ORDER_DATE').desc(), F.col('O_CUSTOMER_SK')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the SQL output below how our model is packaged and called from SQL `MODEL_VERSION_ALIAS!PREDICT(RETURN_RATIO, FREQUENCY) AS TMP_RESULT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "ind_sql = inference_result_sdf.queries['queries'][0]\n",
    "ind_fmtd_sql = os.linesep.join(ind_sql.split(os.linesep)[:1000])\n",
    "print(ind_fmtd_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create & Register Inference-FeatureView to run scheduled Inference\n",
    "\n",
    "We can now define a new Inference Feature View using our Spine and Dataframe reading from our Feature Engineering pipeline.  The FeatureView when created as a Dynamic Table will run to the required refresh_freq and automatically perform incremental inference on new data that arrives through the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "## Create & Register Inference-FeatureView to run scheduled Inference\n",
    "inf_fvname = \"FV_UC01_INFERENCE_RESULT\"\n",
    "inf_fv_version = \"V_1\"\n",
    "\n",
    "inference_features_desc = { \"FREQUENCY\":\"Average yearly order frequency\",\n",
    "                              \"RETURN_RATIO\":\"Average of, Per Order Returns Ratio.  Per order returns ratio : total returns value / total order value\", \n",
    "                              \"OUTPUT_RETURN_ROW_PRICE\":f\"Predicted Return Price for XGB Model (UC01) using Model Registry ({tpcxai_database} MODEL_1) Model ({mv.model_name}) Model-Version({mv.version_name})  Model Comment ({mv.comment}\"}\n",
    "\n",
    "try:\n",
    "   fv_uc01_inference_result = fs.get_feature_view(name= inf_fvname, version= inf_fv_version)\n",
    "except:\n",
    "   fv_uc01_inference_result = FeatureView(\n",
    "         name= inf_fvname, \n",
    "         entities=[customer_entity], \n",
    "         feature_df=inference_result_sdf,\n",
    "         refresh_freq=\"60 minute\",\n",
    "         refresh_mode=\"INCREMENTAL\",\n",
    "         desc=\"Inference Result from kmeans model for Use Case 01\").attach_feature_desc(inference_features_desc)\n",
    "   \n",
    "   fv_uc01_inference_result = fs.register_feature_view(\n",
    "         feature_view=fv_uc01_inference_result, \n",
    "         version= inf_fv_version, \n",
    "         block=True\n",
    "   )\n",
    "   print(f\"Inference Feature View : fv_uc01_inference_result_{inf_fv_version} created\")   \n",
    "else:\n",
    "   print(f\"Inference Feature View : fv_uc01_inference_result_{inf_fv_version} already created\")\n",
    "finally:\n",
    "   fs_serving_fviews = fs.list_feature_views().filter(F.col(\"NAME\") == inf_fvname ).sort(F.col(\"VERSION\").desc())\n",
    "   fs_serving_fviews.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "fv_uc01_inference_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fv_uc01_inference_result.feature_df.sort(F.col(\"LATEST_ORDER_DATE\").desc()).show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLEAN UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "formatted_time = datetime.now(ZoneInfo(\"Australia/Melbourne\")).strftime(\"%A, %B %d, %Y %I:%M:%S %p %Z\")\n",
    "\n",
    "print(f\"The last run time in Melbourne is: {formatted_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-snowpark_df_ml_fs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
