{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version: 0.0.2  Updated date: 07/05/2024\n",
    "Conda Environment : py-snowpark_df_ml_fs-1.15.0_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Snowflake Feature Store\n",
    "We will use the Use-Case to show how Snowflake Feature Store (and Model Registry) can be used to maintain & store features, retrieve them for training and perform micro-batch inference.\n",
    "\n",
    "In the development (TRAINING) enviroment we will \n",
    "- create FeatureViews in the Feature Store that maintain the required customer-behaviour features.\n",
    "- use these Features to train a model, and save the model in the Snowflake model-registry.\n",
    "- plot the clusters for the trained model to visually verify. \n",
    "\n",
    "In the production (SERVING) environment we will\n",
    "- re-create the FeatureViews on production data\n",
    "- generate an Inference FeatureView that uses the saved model to perform incremental inference\n",
    "\n",
    "# Feature Engineering & Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Python packages\n",
    "import os\n",
    "import json\n",
    "import timeit\n",
    "\n",
    "# SNOWFLAKE\n",
    "# Snowpark\n",
    "from snowflake.snowpark import Session, DataFrame, Window, WindowSpec, Row\n",
    "from feature_engineering_fns import uc01_load_data, uc01_pre_process_v2\n",
    "import snowflake.snowpark.functions as F\n",
    "import snowflake.snowpark.types as T\n",
    "\n",
    "# Snowflake Feature Store\n",
    "from snowflake.ml.feature_store import (\n",
    "    FeatureView,\n",
    "    Entity)\n",
    "\n",
    "# COMMON FUNCTIONS\n",
    "from useful_fns import check_and_update, dataset_check_and_update, create_ModelRegistry, create_FeatureStore, create_SF_Session, get_spine_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Snowflake connection and database parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schemas\n",
    "tpcxai_training_schema     = 'TRAINING'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "fs_qs_role, tpcxai_database, tpcxai_training_schema, session, warehouse_env = create_SF_Session(tpcxai_training_schema, role=\"ACCOUNTADMIN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL DEVELOPMENT\n",
    "* Create Snowflake Model-Registry\n",
    "* Create Snowflake Feature-Store\n",
    "* Establish and Create CUSTOMER Entity in the development Snowflake FeatureStore\n",
    "* Create Source Data references and perform basic data-cleansing\n",
    "* Create & Run Preprocessing Function to create features\n",
    "* Create FeatureView_Preprocess from Preprocess Dataframe SQL\n",
    "* Create training data from FeatureView_Preprocess (asof join)\n",
    "* Create & Fit Snowpark-ml pipeline \n",
    "* Save model in Model Registry\n",
    "* 'Verify' and approve model\n",
    "* Create new FeatureView_Model_Inference with Transforms UDF + KMeans model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Set the Schema\n",
    "tpcxai_schema = tpcxai_training_schema\n",
    "\n",
    "# Create/Reference Snowflake Model Registry - Common across Environments\n",
    "mr = create_ModelRegistry(session, tpcxai_database, 'MODEL_1')\n",
    "\n",
    "# Create/Reference Snowflake Feature Store for Training (Development) Environment\n",
    "fs = create_FeatureStore(session, tpcxai_database, f'''_{tpcxai_schema}_FEATURE_STORE''', warehouse_env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.dataset import Dataset\n",
    "ds = Dataset.load(session=session, name='TPCXAI_SF0001_QUICKSTART_INC._TRAINING_FEATURE_STORE.UC01_TRAINING')\n",
    "ds.fully_qualified_name, ds.list_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.dataset import load_dataset\n",
    "training_dataset_sdf_v1 = load_dataset(session, 'TPCXAI_SF0001_QUICKSTART_INC._TRAINING_FEATURE_STORE.UC01_TRAINING', 'V_1')\n",
    "training_dataset_sdf_v1 = training_dataset_sdf_v1.read.to_snowpark_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Display some sample data\n",
    "training_dataset_sdf_v1.sort('O_CUSTOMER_SK').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_sdf_v1.to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Snowpark-ML Transforms & Model using Fileset training data\n",
    "\n",
    "We need to fit the transformer over the training Fileset to ensure we are using the same input global values for transforming and training, and later inference with the model.\n",
    "\n",
    "The transforms here are model-specific and persisted within the model-pipeline, and not stored in the Feature Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(training_dataset_sdf):\n",
    "    weights = [0.6, 0.4]\n",
    "    training_dataset_sdf = training_dataset_sdf.with_column(\"FREQUENCY\", F.round(F.col(\"FREQUENCY\"), 3))\n",
    "    training_dataset_sdf = training_dataset_sdf.with_column(\"RETURN_RATIO\", F.round(F.col(\"RETURN_RATIO\"), 3))\n",
    "    training_dataset_sdf = training_dataset_sdf.with_column(\"RETURN_ROW_PRICE\", F.round(F.col(\"RETURN_ROW_PRICE\"), 3))\n",
    "    training_dataset_sdf = training_dataset_sdf.select(['RETURN_RATIO', 'FREQUENCY', 'RETURN_ROW_PRICE'])\n",
    "\n",
    "    train_df, test_df = training_dataset_sdf.random_split(weights, seed=42) # Using a seed for reproducibility\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#### Use-Case 01 - Specific Packages\n",
    "# from sklearn.pipeline import Pipeline as sml_Pipeline\n",
    "from snowflake.ml.modeling.pipeline.pipeline import Pipeline as sml_Pipeline\n",
    "# from sklearn.preprocessing import MinMaxScaler as sml_MinMaxScaler\n",
    "from snowflake.ml.modeling.preprocessing.min_max_scaler import MinMaxScaler as sml_MinMaxScaler\n",
    "# from sklearn.compose import ColumnTransformer as sml_ColumnTransformer\n",
    "from snowflake.ml.modeling.compose.column_transformer import ColumnTransformer as sml_ColumnTransformer\n",
    "\n",
    "# from xgboost import XGBRegressor\n",
    "from snowflake.ml.modeling.xgboost.xgb_regressor import XGBRegressor as sml_XGBRegressor\n",
    "import pandas as pd\n",
    "from snowflake.snowpark.dataframe import DataFrame\n",
    "def uc01_train(featurevector: DataFrame, n_estimators=100):\n",
    "    feature_cols = ['RETURN_RATIO', 'FREQUENCY'] \n",
    "    mms_output_cols = ['RETURN_RATIO', 'FREQUENCY', 'RETURN_ROW_PRICE']\n",
    "       \n",
    "    target_col = \"RETURN_ROW_PRICE\"\n",
    "\n",
    "    # Create preprocessing steps\n",
    "    preprocessor = sml_ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('NUM', sml_MinMaxScaler(), feature_cols)\n",
    "        ],\n",
    "        remainder='passthrough',\n",
    "        output_cols=mms_output_cols\n",
    "    )\n",
    "\n",
    "    # Create pipeline\n",
    "    pipeline = sml_Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', sml_XGBRegressor(\n",
    "                random_state=42,\n",
    "                n_estimators=n_estimators,\n",
    "                input_cols=feature_cols,\n",
    "                label_cols=[target_col],\n",
    "                output_cols=['PREDICTION']\n",
    "            )\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    model = pipeline.fit(featurevector)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Use-Case 01 - Specific Packages\n",
    "from sklearn.pipeline import Pipeline as sk_Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler as sk_MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer as sk_ColumnTransformer\n",
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "\n",
    "def uc01_train_sklearn(featurevector: pd.DataFrame, n_estimators=100):\n",
    "    feature_cols = ['RETURN_RATIO', 'FREQUENCY'] \n",
    "    mms_output_cols = ['RETURN_RATIO', 'FREQUENCY', 'RETURN_ROW_PRICE']\n",
    "       \n",
    "    target_col = \"RETURN_ROW_PRICE\"\n",
    "\n",
    "    # Create preprocessing steps\n",
    "    preprocessor = sk_ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('NUM', sk_MinMaxScaler(), feature_cols)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    # Create pipeline\n",
    "    pipeline = sk_Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', XGBRegressor(\n",
    "                random_state=42,\n",
    "                n_estimators=n_estimators\n",
    "            )\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    X = featurevector[feature_cols]\n",
    "    y = featurevector[[target_col]]\n",
    "    model = pipeline.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "## Fit the Model\n",
    "model_name = \"MODEL_1.UC01_SNOWFLAKEML_RF_REGRESSOR_MODEL\"\n",
    "train_df, test_df = train_test_split(training_dataset_sdf_v1)\n",
    "train_pd_df = train_df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = uc01_train(train_df)\n",
    "train_sklern_result = uc01_train_sklearn(train_pd_df)\n",
    "\n",
    "# Check for the latest version of this model in registry, and increment version\n",
    "mr_df = mr.show_models()\n",
    "model_version = check_and_update(mr_df, model_name)\n",
    "print('model version:\\t',model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_df.limit(10)\n",
    "sample_pd = train_pd_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Model to the Model Registry\n",
    "model_base = mr.log_model(\n",
    "    model= train_result,\n",
    "    model_name= model_name,\n",
    "    comment=\"TPCXAI USE CASE 01 - XGB Regressor\",\n",
    "    sample_input_data=sample,\n",
    "    # options= {\n",
    "    #     \"enable_explainability\": True,\n",
    "    #     \"method_options\": {\"predict\": {\"case_sensitive\": True}, \"explain\": {\"case_sensitive\": True}}\n",
    "    # }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Model to the Model Registry\n",
    "model_sklearn = mr.log_model(\n",
    "    model= train_sklern_result,\n",
    "    model_name= model_name + \"sklearn\",\n",
    "    comment=\"TPCXAI USE CASE 01 - XGB Regressor\",\n",
    "    sample_input_data=sample_pd[['RETURN_RATIO', 'FREQUENCY']],\n",
    "    options= {\n",
    "        \"enable_explainability\": True,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and set default for latest version of the model\n",
    "m = mr.get_model(model_name)\n",
    "latest_version = m.show_versions().iloc[-1]['name']\n",
    "mv = m.version(latest_version)\n",
    "m.default = latest_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We noticed the feature wasn't correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_tbl = '.'.join([tpcxai_database, tpcxai_schema,'ORDERS'])\n",
    "order_sdf = session.table(order_tbl)\n",
    "customer_entity = fs.get_entity(\"ORDER\")\n",
    "# Tables\n",
    "line_item_tbl            = '.'.join([tpcxai_database, tpcxai_schema,'LINEITEM'])\n",
    "order_returns_tbl        = '.'.join([tpcxai_database, tpcxai_schema,'ORDER_RETURNS'])\n",
    "# Snowpark Dataframe\n",
    "line_item_sdf            = session.table(line_item_tbl)\n",
    "order_returns_sdf        = session.table(order_returns_tbl)\n",
    "raw_data = uc01_load_data(order_sdf, line_item_sdf, order_returns_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_df(spine_sdf, feature_view):\n",
    "    dataset_name = 'UC01_TRAINING_V2'\n",
    "    dataset_version = dataset_check_and_update(session, dataset_name)\n",
    "    # Generate_Dataset\n",
    "    training_dataset = fs.generate_dataset( \n",
    "        name = dataset_name,\n",
    "        version = dataset_version,\n",
    "        spine_df = spine_sdf, \n",
    "        features = [feature_view], \n",
    "        spine_timestamp_col = 'ASOF_DATE'\n",
    "        )                                     \n",
    "    # Create a snowpark dataframe reference from the Dataset\n",
    "    training_dataset_sdf = training_dataset.read.to_snowpark_dataframe()\n",
    "    \n",
    "    return training_dataset_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data_v2 = uc01_pre_process_v2(raw_data)\n",
    "ppd_fv_name    = \"FV_UC01_PREPROCESS\"\n",
    "ppd_fv_version = \"V_2\"\n",
    "# Define descriptions for the FeatureView's Features.  These will be added as comments to the database object\n",
    "preprocess_features_desc = {  \"FREQUENCY\":\"Average yearly order frequency\",\n",
    "                              \"RETURN_RATIO\":\"Average of, Per Order Returns Ratio.  Per order returns ratio : total returns value / total order value\" }\n",
    "\n",
    "# Create the FeatureView instance\n",
    "fv_uc01_preprocess_instance = FeatureView(\n",
    "    name=ppd_fv_name, \n",
    "    entities=[customer_entity], \n",
    "    feature_df=preprocessed_data_v2,      # <- We can use the snowpark dataframe as-is from our Python\n",
    "    # feature_df=preprocessed_data_v2.queries['queries'][0],    # <- Or we can use SQL, in this case linted from the dataframe generated SQL to make more human readable\n",
    "    timestamp_col=\"LATEST_ORDER_DATE\",\n",
    "    refresh_freq=\"60 minute\",            # <- specifying optional refresh_freq creates FeatureView as Dynamic Table, else created as View.\n",
    "    desc=\"Features to support Use Case 01\").attach_feature_desc(preprocess_features_desc)\n",
    "\n",
    "# Register the FeatureView instance.  Creates  object in Snowflake\n",
    "fv_uc01_preprocess_v2 = fs.register_feature_view(\n",
    "    feature_view=fv_uc01_preprocess_instance, \n",
    "    version=ppd_fv_version, \n",
    "    block=True,     # whether function call blocks until initial data is available\n",
    "    overwrite=False # whether to replace existing feature view with same name/version\n",
    ")\n",
    "spine_sdf = get_spine_df(fv_uc01_preprocess_v2)\n",
    "training_dataset_sdf_v2 = generate_training_df(spine_sdf, fv_uc01_preprocess_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit the Model\n",
    "model_name = \"MODEL_1.UC01_SNOWFLAKEML_XGB_REGRESSOR_MODEL_V2\"\n",
    "train_df_v2, test_df_v2 = train_test_split(training_dataset_sdf_v2)\n",
    "train_result = uc01_train(train_df_v2)\n",
    "\n",
    "# Check for the latest version of this model in registry, and increment version\n",
    "mr_df = mr.show_models()\n",
    "model_version = check_and_update(mr_df, model_name)\n",
    "print('model version:\\t',model_version)\n",
    "# Save the Model to the Model Registry\n",
    "sample = train_df.to_pandas().head(10)\n",
    "# Save the Model to the Model Registry\n",
    "mv_kmeans = mr.log_model(\n",
    "    model= train_result,\n",
    "    model_name= model_name,\n",
    "    version_name= model_version,\n",
    "    comment=\"TPCXAI USE CASE 01 - XGB Regressor\",\n",
    "    sample_input_data=sample,\n",
    "    options= {\n",
    "        # \"enable_explainability\": True\n",
    "    }\n",
    ")\n",
    "# Get and set default for latest version of the model\n",
    "m = mr.get_model(model_name)\n",
    "latest_version = m.show_versions().iloc[-1]['name']\n",
    "mv = m.version(latest_version)\n",
    "m.default = latest_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"MODEL_1.UC01_SNOWFLAKEML_XGB_REGRESSOR_MODEL_V2\"\n",
    "train_result = uc01_train(train_df_v2, n_estimators = 110)\n",
    "# Check for the latest version of this model in registry, and increment version\n",
    "mr_df = mr.show_models()\n",
    "model_version = check_and_update(mr_df, model_name)\n",
    "print('model version:\\t',model_version)\n",
    "# Save the Model to the Model Registry\n",
    "sample = train_df.to_pandas().head(10)\n",
    "# Save the Model to the Model Registry\n",
    "mv_kmeans = mr.log_model(\n",
    "    model= train_result,\n",
    "    model_name= model_name,\n",
    "    version_name= model_version,\n",
    "    comment=\"TPCXAI USE CASE 01 - XGB Regressor\",\n",
    "    sample_input_data=sample,\n",
    "    options= {\n",
    "        # \"enable_explainability\": True\n",
    "    }\n",
    ")\n",
    "# Get and set default for latest version of the model\n",
    "m = mr.get_model(model_name)\n",
    "latest_version = m.show_versions().iloc[-1]['name']\n",
    "mv = m.version(latest_version)\n",
    "m.default = latest_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate_Dataset\n",
    "training_set_sdf = fs.generate_training_set(\n",
    "                                        spine_df = spine_sdf, \n",
    "                                        features = [fv_uc01_preprocess_v2], \n",
    "                                        spine_timestamp_col = 'ASOF_DATE'\n",
    "                                        )                                     \n",
    "# Display some sample data\n",
    "training_set_sdf.sort('O_CUSTOMER_SK').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.show_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"MODEL_1.UC01_SNOWFLAKEML_XGB_REGRESSOR_MODEL_V2\"\n",
    "train_result = uc01_train(train_df_v2, n_estimators = 110)\n",
    "# Check for the latest version of this model in registry, and increment version\n",
    "mr_df = mr.show_models()\n",
    "model_version = check_and_update(mr_df, model_name)\n",
    "print('model version:\\t',model_version)\n",
    "# Save the Model to the Model Registry\n",
    "mv_kmeans = mr.log_model(\n",
    "    model= train_result,\n",
    "    model_name= model_name,\n",
    "    version_name= model_version,\n",
    "    comment=\"TPCXAI USE CASE 01 - XGB Regressor\",\n",
    "    sample_input_data=sample,\n",
    "    options= {\n",
    "        # \"enable_explainability\": True\n",
    "    }\n",
    ")\n",
    "# Get and set default for latest version of the model\n",
    "m = mr.get_model(model_name)\n",
    "latest_version = m.show_versions().iloc[-1]['name']\n",
    "mv = m.version(latest_version)\n",
    "m.default = latest_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the model clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will check the model clusters derived from the model.  We create an inference Function using the Snowflake Model Registry.  This packages our model as a Python function which enables access from [Python](https://docs.snowflake.com/developer-guide/snowpark-ml/model-registry/overview#calling-model-methods) or directly from [SQL](https://docs.snowflake.com/sql-reference/commands-model#label-snowpark-model-registry-model-methods).  This allows the model to be used directly for prediction within our Feature Engineering pipeline, by creating an inference Feature View."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def uc01_serve(featurevector, model) -> DataFrame :\n",
    "    clusters = model.run(featurevector, function_name=\"predict\")    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Create Spine\n",
    "inference_spine_sdf =  fv_uc01_preprocess_v2.feature_df.group_by('O_CUSTOMER_SK').agg(F.max('LATEST_ORDER_DATE').as_('ASOF_DATE'))\n",
    "\n",
    "# Generate_Dataset\n",
    "inference_dataset_sdf = fs.retrieve_feature_values(spine_df = inference_spine_sdf, features = [fv_uc01_preprocess_v2],  spine_timestamp_col = 'ASOF_DATE' )\n",
    "#inference_dataset_sdf = fs.read_feature_view(fv_uc01_preprocess_v2)\n",
    "\n",
    "start = timeit.default_timer()#\n",
    "# serve_result = uc01_serve(training_dataset_sdf, train_result['MODEL'])#\n",
    "inference_result_sdf = uc01_serve(inference_dataset_sdf, mv)#\n",
    "end = timeit.default_timer()#\n",
    "serve_time = end - start#\n",
    "print('serve time:\\t', serve_time)#\n",
    "inference_result_sdf.show()\n",
    "# inference_sample_sdf = inference_result_sdf.sample(n = 10000)\n",
    "# inference_sample_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_result_sdf.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting unique labels\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt_df = inference_result_sdf.select(F.col(\"PREDICTION\"), F.col(\"RETURN_ROW_PRICE\")).to_pandas()\n",
    "y_predicted = plt_df['PREDICTION']\n",
    "y_actual = plt_df['RETURN_ROW_PRICE']\n",
    "# --- 1. Calculate Residuals ---\n",
    "# You need y_actual and y_predicted from the previous example\n",
    "residuals = y_actual - y_predicted\n",
    "\n",
    "# --- 2. Create the Residual Plot ---\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.scatter(y_predicted, residuals, edgecolors=(0, 0, 0), alpha=0.8)\n",
    "\n",
    "# --- 3. Add a Horizontal Line at Zero ---\n",
    "# This line represents zero error\n",
    "ax.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "\n",
    "# --- 4. Add Labels and Title ---\n",
    "ax.set_xlabel('Predicted Values', fontsize=12)\n",
    "ax.set_ylabel('Residuals (Actual - Predicted)', fontsize=12)\n",
    "ax.set_title('Residual Plot', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the query that contains our inference function.  It makes use of the SQL API for Model registry to call the inference function `MODEL_VERSION_ALIAS!PREDICT(RETURN_RATIO, FREQUENCY) AS TMP_RESULT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "ind_sql = inference_result_sdf.queries['queries'][0]\n",
    "ind_fmtd_sql = os.linesep.join(ind_sql.split(os.linesep)[:1000])\n",
    "print(ind_fmtd_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLEAN UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "formatted_time = datetime.now(ZoneInfo(\"Australia/Melbourne\")).strftime(\"%A, %B %d, %Y %I:%M:%S %p %Z\")\n",
    "\n",
    "print(f\"The last run time in Melbourne is: {formatted_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-snowpark_df_ml_fs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
